# Task 2A : Creating Word Embeddings

The goal of this task is to experiment and create word embeddings based on the given sentences and the context. 

The baseline model provided is itself is done with TF-IDF, one of the most primitive ways of creating embeddings out of a given corpora of text. 

> The NewsQA Dataset can be used as provided in the references of Task 2C

Firstly, it should be very evident by going through the notebook that the embedding process is not scalable, and is subject to being skewed. You are expected to try out improvements over the current method of embedding through code and document the same. For each set of embeddings created, you are expected to create a csv file with (word, embedding) pairs and upload the same in your repository.

Different forms of experimentation and architectural model building are encouranged irrelevant of the performance of the model. Novel Ideation or Smart Experimentaion is highly appreciated.

## References :
- [What are word embeddings?](https://machinelearningmastery.com/what-are-word-embeddings/)
- [Guide to generating word embeddings](https://www.tensorflow.org/text/guide/word_embeddings)
- [Introduction to Word Embeddings](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa)
